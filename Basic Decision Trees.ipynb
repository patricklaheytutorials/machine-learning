{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<script>\n",
    "function code_toggle() {\n",
    "  if (code_shown) {\n",
    "    $('div.input').hide('500');\n",
    "    $('#toggleButton').val('Show Code')\n",
    "  } else {\n",
    "    $('div.input').show('500');\n",
    "    $('#toggleButton').val('Hide Code')\n",
    "  }\n",
    "}\n",
    "$( document ).ready(function(){\n",
    "  code_shown=false;\n",
    "  $('div.input').hide()\n",
    "});\n",
    "</script>\n",
    "<form action=\"javascript:code_toggle()\"><input type=\"submit\" id=\"toggleButton\" value=\"Show Code\"></form>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Decision Trees\n",
    "## The Problem\n",
    "Let's start with a concrete example of the type of problem we are going to discuss.  Supposed that you are a bird watcher and you have been tracking if your favorite Barn Swallow shows up during lunch time.  You have been keeping track of a number of different variables (in machine learning we call these \"features\") every morning along with whether or not your friend shows up during lunch.  The features you record are:\n",
    "\n",
    "| Feature Name | Possible Values      |\n",
    "|--------------|----------------------|\n",
    "| Outlook      |Sunny, Overcast, Rain |\n",
    "| Temperature  |Cool, Mild, Hot       |\n",
    "| Humidity     |High, Normal          |\n",
    "| Windy        |True, False           |\n",
    "\n",
    "This is called \"categorical data\" because each feature takes on a single value from a finite set of possible values.  Categorical data is often created by grouping data that is otherwise continuous (e.g. Temperature gets grouped into Cool, Mild or Hot based on some criterion).\n",
    "\n",
    "The goal of this type of machine learning problem is to determine, based on a particular measurement or \"instance\" of the features values (i.e. Outlook, Temperature, Humidity, Windy), whether our bird friend will show up.  This is called a binary classification problem because we are classifying instances (e.g. Outlook=Overcast, Temperature=Hot, Humidity=High, Windy=True) as either thumbs up (the bird shows up) or thumbs down (the bird does not show up).\n",
    "\n",
    "## A Solution\n",
    "The mathematical view of the above problem is that we are looking for a function.  Functions map each input to one and only one output.  The function in this case maps (Outlook, Temperature, Humidity, Windy) to thumbs up or thumbs down.  How many different possible functions are there to choose from?\n",
    "\n",
    "There are $3 \\cdot 3 \\cdot 2 \\cdot 2 = 36$ possible inputs (Outlook has 3 possibilities, Temperature has 3, Humidity has 2 and Windy has 2 possibilities).  For each input the function can evaluate to one of two values (thumbs up or thumbs down).  Thus there are $2^{36}=68,719,476,736$ possible functions associated with this simple problem.\n",
    "\n",
    "### A Little Theory\n",
    "Does it surprise you that a problem this simple has so many different possible solutions?  This is just a fact of life for machine learning practitioners.  Consider the case of a problem with $N$ features each of which can take on 2 values in a binary classification problem.  In this case, the number of possible solutions/functions is\n",
    "\n",
    "$$2^{2^{N}}$$\n",
    "\n",
    "(each of the $2^{N}$ possible inputs has 2 possible outputs).  Note that the size of the solution space grows like a double exponential with the number of features involved.  Clearly a brute force search of the solution space is not viable.\n",
    "\n",
    "It is useful to note that even just to explicitly represent the single \"correct\" function that we are after requires exponential memory - we need to remember the output for each of the $2^{N}$ possible inputs.  Of course if we know the correct output for every input we don't have a learning problem.  We have (in practice at least) a compression problem.  If we choose to use lossy compression then we areÂ effectively approximating a known function.  Neither of these is learning.\n",
    "\n",
    "### What is Learning?\n",
    "There are a number of different types of learning problems.  For the most part they all involve trying to approximate a function based on instance data (sets of features) which may (supervised learning) or may not (unsupervised learning) have \"correct\" outputs associated with them.\n",
    "\n",
    "Typically the amount of data available to learn from is a tiny fraction of the set of all possible inputs (i.e. all possible combinations of feature values).  Given this, it is reasonable ask if \"learning\" is even possible.  How can we possibly know what value a function will take on without observing what it actually evaluates to?  The answers is that it is not possible to learn \"pathological\" functions but it is possible to approximate functions that exhibit \"large scale\" patterns in their output which can be detected from sparse sampling.  We won't attempt here to rigorously defined the conditions under which \"generalization\" is possible.  We will simply state that if a data set of interest contains patterns that generalize to the full domain it is possible (at least in theory) to learn and exploit them.\n",
    "\n",
    "### A Function?  Really?\n",
    "It is quite reasonable to question whether a function is really what we are after here.  Surely whether or not our friendly Barn Swallow shows up depends on more than just some crude measures of the weather!  This is most likely true and is often true in real world cases as well.  There may be \"unobserved\" features such as the proximity of cats at lunch time or numerous other factors related to Barn Swallow psychology.  The best we can really hope for is to approximate the \"real\" underlying function using just the data that is available.  There will be \"modeling\" error that can't be avoided.  This is not quite as bad as it sounds since approximation is probably the best we can hope for any way given the sparseness of the data typically available.  We will come back to this point later when we discuss bias and variance.\n",
    "\n",
    "### How Do We Approximate the Function?\n",
    "Before we tackle the question of how to approximate the function we are after it is useful to consider if it possible to always represent the function exactly if we have a complete set of inputs and outputs.  In machine learning choosing the manner in which the function will be represented is called choosing the \"hypothesis space.\"  The specific function that is choosen based on the data is called the \"hypothesis.\"   The hypothesis is restricted to be a member (in a set theoretic sense) of the hypothesis space.  In general the choice of hypothesis space can dramaticaly impact the quality of the resulting hypothesis (i.e. how well we are able to approximate the function we are after) and the amount of work necessary to pick the best hypothesis.  In the case at hand - supervised learning of binary classification over categorical data - we are in luck since a simple hypothesis space is both fairly obvious and capable of exactly representing the function we are after.\n",
    "\n",
    "A simple way to represent a function that captures the observed instances is simply a series of if-then statements:\n",
    "\n",
    "```python\n",
    "if   outlook=='Sunny' and temperature=='Hot' and humidity=='High' and windy==False: return 'thumbs down'\n",
    "elif outlook=='Sunny' and temperature=='Hot' and humidity=='High' and windy==True:  return 'thumbs down'\n",
    "#...\n",
    "```\n",
    "\n",
    "It should be fairly obvious that this hypothesis space (a series of if-then statements over all the features) is capable of caturing the function we are after exactly **if** we have the complete set of inputs and outputs.  The problem, as we mentioned before, however is that this is not learning.  If we already have all the inputs and outputs there is nothing to learn!  If we just use this \"series of rules\" approach on sparse input we are left with no ability to generalize to inputs that have not been observed.  The if-then approach is promising however.  Consider the following instance data from which we wish to learn an approximate classifier function:\n",
    "\n",
    "| Outlook  | Temperature | Humidity | Windy | Barn Swallow? |\n",
    "|----------|-------------|----------|-------|---------------|\n",
    "| Sunny    | Hot         | High     | False | thumbs down   |\n",
    "| Sunny    | Hot         | High     | True  | thumbs down   |\n",
    "| Overcast | Hot         | High     | False | thumbs up     |\n",
    "| Rain     | Mild        | High     | False | thumbs up     |\n",
    "| Rain     | Cool        | Normal   | False | thumbs up     |\n",
    "| Rain     | Cool        | Normal   | True  | thumbs down   |\n",
    "| Overcast | Cool        | Normal   | True  | thumbs up     |\n",
    "| Sunny    | Mild        | High     | False | thumbs down   |\n",
    "| Sunny    | Cool        | Normal   | False | thumbs up     |\n",
    "| Rain     | Mild        | Normal   | False | thumbs up     |\n",
    "| Sunny    | Mild        | Normal   | True  | thumbs up     |\n",
    "| Overcast | Mild        | High     | True  | thumbs up     |\n",
    "| Overcast | Hot         | Normal   | False | thumbs up     |\n",
    "| Rain     | Mild        | High     | True  | thumbs down   |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"\"><tr style=\"\"><td style=\"\">f</td><td style=\"\">x</td><td style=\"\">y</td><td style=\"\">z</td></tr><tr style=\"\"><td style=\"\">0.023432</td><td style=\"\">1</td><td style=\"\">6</td><td style=\"\">testing</td></tr><tr style=\"\"><td style=\"\">0.234321</td><td style=\"\">2</td><td style=\"\">4</td><td style=\"\">pretty</td></tr><tr style=\"\"><td style=\"\">0.5555</td><td style=\"\">3</td><td style=\"\">3</td><td style=\"\">tables</td></tr></table>"
      ],
      "text/plain": [
       "<itable.itable.PrettyTable at 0x1066b30b8>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import itable\n",
    "# Create an example data frame\n",
    "df = pd.DataFrame({\"x\":[1,2,3], \"y\":[6,4,3], \"z\":[\"testing\",\"pretty\",\"tables\"], \"f\":[0.023432, 0.234321,0.5555]})\n",
    "# Display a simple table\n",
    "itable.PrettyTable(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<center><table style=\"color: black;border: 1px solid black;\"><tr style=\"color: black;border: 1px solid black;\"><td style=\"color: black;background-color: lightgray;font-weight: bold;\">f</td><td style=\"color: black;background-color: lightgray;font-weight: bold;\">x</td><td style=\"color: black;background-color: lightgray;font-weight: bold;\">y</td><td style=\"color: black;background-color: lightgray;font-weight: bold;\">z</td></tr><tr style=\"color: black;border: 1px solid black;\"><td style=\"color: black;border: 1px solid black;\">0.023432</td><td style=\"color: black;border: 1px solid black;\">1</td><td style=\"color: black;border: 1px solid black;\">6</td><td style=\"color: black;border: 1px solid black;\">testing</td></tr><tr style=\"color: black;border: 1px solid black;\"><td style=\"color: black;border: 1px solid black;\">0.234321</td><td style=\"color: black;border: 1px solid black;\">2</td><td style=\"color: black;border: 1px solid black;\">4</td><td style=\"color: black;border: 1px solid black;\">pretty</td></tr><tr style=\"color: black;border: 1px solid black;\"><td style=\"color: black;border: 1px solid black;\">0.5555</td><td style=\"color: black;border: 1px solid black;\">3</td><td style=\"color: black;border: 1px solid black;\">3</td><td style=\"color: black;border: 1px solid black;\">tables</td></tr></table></center>"
      ],
      "text/plain": [
       "<itable.itable.PrettyTable at 0x10668ce10>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Give it a theme and center the table\n",
    "itable.PrettyTable(df, tstyle=itable.TableStyle(theme=\"theme1\"), center=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
